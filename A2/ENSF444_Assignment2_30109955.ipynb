{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 2: Linear Models and Validation Metrics</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = Dominic Choi\n",
        "* **UCID** = 30109955\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.</font>\n",
        "\n",
        "You can use the Table of Content on the left side of this notebook to efficiently navigate within this documents.\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|           **Part 2: Regression**           |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    0.5    |\n",
        "| Step 3: Implement Machine Learning   Model |     1     |\n",
        "|            Step 4: Validate Mode           |     1     |\n",
        "|          Step 5: Visualize Results         |     1     |\n",
        "|                  Questions                 |     2     |\n",
        "|             Process Description            |     4     |\n",
        "|  **Part 3:   Observations/Interpretation** |   **3**   |\n",
        "|           **Part 4: Reflection**           |   **2**   |\n",
        "|                  **Total**                 |   **30**  |\n",
        "|                                            |           |\n",
        "|                  **Bonus**                 |           |\n",
        "|         **Part 5: Bonus Question**         |   **4**   |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 1: Classification (14.5 marks total)**\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|                  **Total**                 |  **14.5** |\n",
        "\n",
        "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "## **Step 0:** Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X is size 4600, and of type <class 'pandas.core.frame.DataFrame'>\n",
            "y is size 4600, and of type <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Import spam dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "# Load the spam dataset\n",
        "X, y = load_spam()\n",
        "\n",
        "# Print the size and type of X and y\n",
        "print(f\"X is size {len(X)}, and of type {type(X)}\")\n",
        "print(f\"y is size {len(y)}, and of type {type(y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "4e7204f5",
      "metadata": {
        "id": "4e7204f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are no missing values in X\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in X\n",
        "missing_values = X.isnull().any().any()\n",
        "\n",
        "if missing_values:\n",
        "    print(\"There are missing values in X\")\n",
        "\n",
        "    # Fill missing values with the mean of each column\n",
        "    X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "    print(\"Missing values filled\")\n",
        "else:\n",
        "    print(\"There are no missing values in X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of X_small: (230, 57)\n",
            "Size of y_small: 230\n"
          ]
        }
      ],
      "source": [
        "# Split the data into a smaller subset containing 5% of the original data\n",
        "X_small, _, y_small, _ = train_test_split(X, y, train_size=0.05, random_state=0)\n",
        "\n",
        "# Print the size of X_small and y_small\n",
        "print(\"Size of X_small:\", X_small.shape)\n",
        "print(\"Size of y_small:\", len(y_small))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implement the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks for steps 3-5)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Data Length  Training Accuracy  Validation Accuracy\n",
            "0       4600.0           0.927174             0.936957\n",
            "1       4600.0           0.614946             0.593478\n",
            "2        230.0           0.934783             0.913043\n"
          ]
        }
      ],
      "source": [
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "\n",
        "# Create DataFrame to store the results\n",
        "results = pd.DataFrame(columns=[\"Data Length\", \"Training Accuracy\", \"Validation Accuracy\"])\n",
        "\n",
        "# Set up the X and y args array\n",
        "    # 1st args: X and y\n",
        "    # 2nd args: Only first two columns of X and y\n",
        "    # 3rd args: X_small and y_small\n",
        "X_arg = [X, X.iloc[:, :2], X_small]\n",
        "y_arg = [y, y, y_small]\n",
        "\n",
        "for i in range(0, 3):\n",
        "    # Instantiate model LogisticRegression(max_iter=2000)\n",
        "    model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_arg[i], y_arg[i], test_size=0.2, random_state=0)\n",
        "\n",
        "    # Implement the machine learning model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate the training and validation accuracy for the three different tests implemented in Step 3\n",
        "    y_train_model = model.predict(X_train)\n",
        "    y_train_accuracy = accuracy_score(y_train, y_train_model)\n",
        "    \n",
        "    y_test_model = model.predict(X_test)\n",
        "    y_test_acc = accuracy_score(y_test, y_test_model)\n",
        "    \n",
        "    results.loc[i] = [len(X_arg[i]), y_train_accuracy, y_test_acc]\n",
        "\n",
        "# Print results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "## **Questions (4 marks)**\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da42eaea",
      "metadata": {},
      "source": [
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "1. The training and validation accuracy are impacted greatly by the amount of definiing features of a sample, while the quantity of samples are not as important. This is shown by the fact that the model trained on the original, and the model trained on only 5% of the original dataset, both still had about 92% Training and Validation Accuracy. This contrasts the model trained on only the first 2 columns, which only had 60% in both training and validation accuracy.\n",
        "\n",
        "2. In this case, a False Positive represents an email that was marked as spam, that was actually not spam. A False Negative represents a spam email that was not caught. A False Negative would be worse, since you may get a malicous spam email in your inbox, compromising your security or personal information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe687f",
      "metadata": {
        "id": "59fe687f"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "1. Where did you source your code?\n",
        "    - Lecture Slides on Model Selection: https://d2l.ucalgary.ca/d2l/le/content/569863/viewContent/6238013/View\n",
        "    - Source from Lecture Slides: https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html\n",
        "    - Github Copilot\n",
        "\n",
        "2. In what order did you complete the steps?\n",
        "    - I followed the order provided in this notebook.\n",
        "\n",
        "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "    - GitHub Copilot uses the code already written to try and predict what is coming next. This is not always exactly what I need, so I have to modify it. This is because Copilot is useful for tasks that are very common; It is better to use its suggestions as a template. Human intervention is necessary for more specialized tasks.\n",
        "\n",
        "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
        "    - I was confused as to what was meant by \"loop to store data,\" until I actually implemented all three, and realized that it was the same code 3 times, just with different datasets. I ended up implementing the loop with an array of the datasets as arguments for train_test_split function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "# **Part 2: Regression (10.5 marks total)**\n",
        "\n",
        "| **Question**                               | **Point** |\n",
        "|--------------------------------------------|-----------|\n",
        "| **Part 2: Regression**                     |           |\n",
        "| Step 1: Data Input                         | 1         |\n",
        "| Step 2: Data Processing                    | 0.5       |\n",
        "| Step 3: Implement Machine Learning   Model | 1         |\n",
        "| Step 4: Validate Mode                      | 1         |\n",
        "| Step 5: Visualize Results                  | 1         |\n",
        "| Questions                                  | 2         |\n",
        "| Process Description                        | 4         |\n",
        "| **Total**                                  | **10.5**  |\n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "6ff2e34f",
      "metadata": {
        "id": "6ff2e34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X is size 1030, and of type <class 'pandas.core.frame.DataFrame'>\n",
            "y is size 1030, and of type <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Import spam dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_concrete\n",
        "# Print size and type of X and y\n",
        "X_con, y_con = load_concrete()\n",
        "\n",
        "print(f\"X is size {len(X_con)}, and of type {type(X_con)}\")\n",
        "print(f\"y is size {len(y_con)}, and of type {type(y_con)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "## **Step 2:** Data Processing (0.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "693c5fa3",
      "metadata": {
        "id": "693c5fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are no missing values in X\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in X\n",
        "missing_values = X_con.isnull().any().any()\n",
        "\n",
        "if missing_values:\n",
        "    print(\"There are missing values in X\")\n",
        "\n",
        "    # Fill missing values with the mean of each column\n",
        "    X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "    print(\"Missing values filled\")\n",
        "else:\n",
        "    print(\"There are no missing values in X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (1 mark)\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "b5041945",
      "metadata": {
        "id": "b5041945"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# First, split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_con, y_con, train_size=0.75, random_state=0)\n",
        "\n",
        "# Import LinearRegression from sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate model LinearRegression()\n",
        "model = LinearRegression()\n",
        "\n",
        "# Implement the machine learning model\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "## **Step 4:** Validate Model (1 mark)\n",
        "\n",
        "Calculate the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "970c038b",
      "metadata": {
        "id": "970c038b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training R^2 Score: 0.6108229424520554\n",
            "Training MSE: 111.35843861132469\n",
            "Validation R^2 Score: 0.6234144623633329\n",
            "Validation MSE 95.90413603680645\n"
          ]
        }
      ],
      "source": [
        "# Training accuracy\n",
        "y_train_model = model.predict(X_train)\n",
        "\n",
        "# Calculate the training and validation accuracy using R^2 score and mean squared error\n",
        "y_train_R2 = model.score(X_train, y_train)\n",
        "y_train_mse = mean_squared_error(y_train, y_train_model)\n",
        "\n",
        "# Print the training accuracy\n",
        "print(\"Training R^2 Score:\", y_train_R2)\n",
        "print(\"Training MSE:\", y_train_mse)\n",
        "\n",
        "# Test accuracy\n",
        "y_test_model = model.predict(X_test)\n",
        "\n",
        "# Calculate the training and validation accuracy using R^2 score and mean squared error\n",
        "y_test_R2 = model.score(X_test, y_test)\n",
        "y_test_mse = mean_squared_error(y_test, y_test_model)\n",
        "\n",
        "# Print the validation accuracy\n",
        "print(\"Validation R^2 Score:\", y_test_R2)\n",
        "print(\"Validation MSE\", y_test_mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "## **Step 5:** Visualize Results (1 mark)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "88d223f3",
      "metadata": {
        "id": "88d223f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           Training Accuracy  Validation Accuracy\n",
            "MSE               111.358439            95.904136\n",
            "R^2 score           0.610823             0.623414\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame to store the results\n",
        "results = pd.DataFrame(columns=[\"Training Accuracy\", \"Validation Accuracy\"], index=[\"MSE\", \"R^2 score\"])\n",
        "\n",
        "# Add Results\n",
        "results[\"Training Accuracy\"] = [y_train_mse, y_train_R2]\n",
        "results[\"Validation Accuracy\"] = [y_test_mse, y_test_R2]\n",
        "\n",
        "# Print results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "## **Questions (2 marks)**\n",
        "1. Did using a linear model produce good results for this dataset? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046514f8",
      "metadata": {},
      "source": [
        "<font color='Green'><b>YOUR ANSWER HERE</b></font>\n",
        "\n",
        "- The linear model produced good results, because the R^2 Score for both training and validation are about 0.61, but the both Training (111) and Validation (95) MSE scores are quite high, which indicates that there are quite a few errors in the model's predicitions. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "<font color='Green'><b>Explain YOUR PROCESS here:</b></font>\n",
        "\n",
        "1. Where did you source your code?\n",
        "    - Same D2L resources as Part 1\n",
        "    - Github Copilot\n",
        "\n",
        "2. In what order did you complete the steps?\n",
        "    - Same order as notebook\n",
        "\n",
        "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "    - Copilot created a template from the previously written code\n",
        "\n",
        "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
        "    - No challenges were encountered for this part, as it was very similar to Part 1, but shorter."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4735104",
      "metadata": {},
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796410e6",
      "metadata": {},
      "source": [
        "<font color='Green'><b>ADD YOUR FINDINGS HERE</b></font>\n",
        "\n",
        "- From both Part 1 and 2, it is clear that the number of features of the dataset have a great effect on the accuracy of the model trained on it. Part 1 had nearly 60 columns, which had helped the model achieve very high accuracies. Part 2 only had 8 features, and the MSE was very high. This shows a correlation between amount of features and accuracy of the model trained on the dataset. \n",
        "\n",
        "- The model in part 2 seems to have high bias, as discussed in the lectures. While the R^2 scores were normal, the MSE scores were ranging between 95 and 110, which indicates that Linear Regression might be too simple to predict accurately; this could be a result of underfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b84eed",
      "metadata": {
        "id": "40b84eed"
      },
      "source": [
        "# **Part 4: Reflection (2 marks)**\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4302eaf0",
      "metadata": {},
      "source": [
        "<font color='Green'><b>ADD YOUR THOUGHTS HERE</b></font>\n",
        "\n",
        "- I liked that this is an application of what we have been doing in the labs, and gives more practice and introduction to Regression and Classification models.\n",
        "- I disliked some of the repetitiveness in the assignment. It felt like I did the same assignment twice.\n",
        "- I found learning more about regression and classifaction models to be interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "# **Part 5: Bonus Question (4 marks)**\n",
        "\n",
        "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "47623d44",
      "metadata": {
        "id": "47623d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chosen alpha for Ridge: 100.0\n",
            "           Training accuracy  Validation accuracy\n",
            "MSE               110.345597            95.625173\n",
            "R^2 score           0.609071             0.636937\n",
            "\n",
            "\n",
            "Chosen alpha for Lasso: 0.001\n",
            "           Training accuracy  Validation accuracy\n",
            "MSE               110.345501            95.634971\n",
            "R^2 score           0.609071             0.636899\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# Instantiate models\n",
        "models = []\n",
        "models.append(RidgeCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100]))\n",
        "models.append(LassoCV(alphas=[0.001, 0.01, 0.1, 1, 10, 100]))\n",
        "\n",
        "# Use the concrete dataset from part 2\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_con, y_con, train_size=0.8, random_state=0)\n",
        "\n",
        "# Implement the machine learning model\n",
        "for model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Training accuracy\n",
        "    y_train_model = model.predict(X_train)\n",
        "    y_train_R2 = model.score(X_train, y_train)\n",
        "    y_train_mse = mean_squared_error(y_train, y_train_model)\n",
        "\n",
        "    # Test accuracy\n",
        "    y_test_model = model.predict(X_test)\n",
        "    y_test_R2 = model.score(X_test, y_test)\n",
        "    y_test_mse = mean_squared_error(y_test, y_test_model)\n",
        "\n",
        "    results = pd.DataFrame(columns=[\"Training accuracy\", \"Validation accuracy\"], index=[\"MSE\", \"R^2 score\"])\n",
        "    results[\"Training accuracy\"] = [y_train_mse, y_train_R2]\n",
        "    results[\"Validation accuracy\"] = [y_test_mse, y_test_R2]\n",
        "\n",
        "    print(f\"Chosen alpha for {str(model)[:5]}:\", model.alpha_)\n",
        "    print(results)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "<font color='Green'><b>ADD YOUR ANSWER HERE</b></font>\n",
        "\n",
        "- For Ridge, the best alpha was 100. \n",
        "- For Lasso, the best alpha was 0.001.\n",
        "\n",
        "- The accuracy scores did not change very much compared to linear in part 2, because Ridge and Lasso are good at preventing overfitting, but not so much when it comes to underfitting. In this case, it was underfitting, so both Ridge and Lasso are not the appropriate solution to improve the scores. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
